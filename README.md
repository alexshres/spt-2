## SPT-2

My small implementation of GPT-2.

Implemented so far:

- [x] Layer Norm
- [x] Token Embedding/Positional Embedding
- [x] Causal Attention
- [x] MLP
- [x] Transformer Blocks
- [x] Full Transformer Module

TODO:
- [] Write tests for each of the implementations above.
- [] Create a distributed trainer class (train on 4x GPUs from vast.ai)
- [] Log and show training graphs from wandb
- [] beam search sampling for inference
- [] KV Cache for faster inference



WIP.